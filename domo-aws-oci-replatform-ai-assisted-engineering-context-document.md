================================================================================
     DOMO AWS‚ÜíOCI REPLATFORM: AI-ASSISTED ENGINEERING CONTEXT DOCUMENT
          Share this with AI coding assistants for optimal code generation
================================================================================
This context document provides your team with:

Practical, copy-paste code examples for OCI
AWS‚ÜíOCI migration patterns specific to Domo's architecture
Common gotchas and their solutions
AI assistant prompts for generating OCI code
Testing frameworks for validation
Cost optimization strategies

Share this with your engineers and they can feed it to AI coding assistants to generate high-quality, OCI-optimized code that follows best practices!

## DOCUMENT PURPOSE
This context document should be provided to AI coding assistants (Claude, Cursor, 
GitHub Copilot, etc.) when working on the Domo AWS‚ÜíOCI migration. It contains:
- OCI-specific patterns and best practices
- AWS‚ÜíOCI service mappings with code examples  
- Common gotchas and their solutions
- Performance and cost optimization patterns
- Migration-specific engineering guidance

## PROJECT CONTEXT

### System Overview
We are migrating Domo's analytics control plane from AWS to OCI while maintaining:
- **Zero downtime** for customer workloads
- **Tundra query engine** (custom MPP analytics engine, 8x faster than Snowflake)
- **S3‚ÜíObject Storage** as the durable data layer
- **Multi-tenant architecture** with logical isolation
- **Fast hydration** from Object Storage to Tundra clusters (target: 3-8 min)

### Key Architecture Principles
1. **Ephemeral Compute, Durable Storage**: Tundra clusters are stateless, all data in Object Storage
2. **Zero-Key Security**: Use OCI Instance Principals everywhere (no access keys)
3. **Flex Everything**: Leverage OCI Flex shapes for dynamic resource sizing
4. **Cost First**: Free egress (10TB/month) is a major win, optimize for this
5. **Hybrid Period**: AWS and OCI will coexist during migration (2-6 months)

### Critical Performance Requirements
- Query latency p95: <1000ms (currently 1200ms on AWS)
- Hydration time: <8 minutes (currently 10-15 min on AWS)
- Cluster startup: <5 minutes
- DNS cutover: <2 minutes (zero downtime)
- Rollback: <5 minutes (if issues detected)

================================================================================
                        PART 1: OCI SERVICE MAPPING
================================================================================

## AWS ‚Üí OCI Direct Replacements
```python
# REFERENCE TABLE: Copy this into your prompts when asking for code

AWS_TO_OCI_MAPPING = {
    # Compute
    "EC2": "OCI Compute",
    "Auto Scaling Groups": "Instance Pools + Autoscaling Config",
    "Lambda": "OCI Functions",
    "ECS Fargate": "OCI Container Instances",
    "EKS": "OKE (Oracle Container Engine for Kubernetes)",
    
    # Storage
    "S3": "OCI Object Storage",
    "EBS": "OCI Block Volumes",
    "EFS": "OCI File Storage Service",
    
    # Database
    "RDS PostgreSQL": "OCI Base Database Service (PostgreSQL)",
    "RDS MySQL": "OCI MySQL Database Service",
    "DynamoDB": "OCI NoSQL Database",
    
    # Networking
    "VPC": "VCN (Virtual Cloud Network)",
    "ALB/NLB": "OCI Load Balancer (Flexible)",
    "NAT Gateway": "NAT Gateway (OCI)",
    "Transit Gateway": "DRG (Dynamic Routing Gateway)",
    "VPC Endpoints": "Service Gateway (free for OCI services)",
    "Direct Connect": "FastConnect",
    
    # Security
    "IAM": "OCI IAM",
    "KMS": "OCI Vault",
    "CloudTrail": "OCI Audit (immutable logs)",
    "Secrets Manager": "OCI Vault (secrets)",
    
    # Monitoring
    "CloudWatch": "OCI Monitoring",
    "CloudWatch Logs": "OCI Logging",
    "X-Ray": "OCI APM (Application Performance Monitoring)",
    
    # Caching
    "ElastiCache": "OCI Cache (Redis/Memcached)",
    
    # Data Processing
    "EMR": "OCI Data Flow (Spark)",
    "Glue": "OCI Data Integration",
    "Kinesis": "OCI Streaming",
    "SQS": "OCI Queue",
    "EventBridge": "OCI Events Service",
    
    # AI/ML
    "Bedrock": "OCI Generative AI Service",
    "SageMaker": "OCI Data Science",
}

# IMPORTANT NOTES:
# - OCI Instance Principals replace AWS access keys (use everywhere)
# - OCI Service Gateway provides FREE access to Object Storage (no NAT/egress)
# - OCI Flex shapes can resize without reboot (major win vs AWS)
# - OCI Object Storage first 10TB egress/month is FREE (vs AWS $922/TB)
# - OCI Audit logs are immutable (better compliance than CloudTrail)
```

================================================================================
                    PART 2: OCI INSTANCE PRINCIPALS PATTERN
================================================================================

## Critical Pattern: Zero-Key Authentication

**ALWAYS use Instance Principals instead of API keys. This is the #1 OCI differentiator.**

### AWS (Bad - Uses Access Keys)
```python
import boto3

# ‚ùå BAD: Hard-coded credentials or ENV vars
s3_client = boto3.client(
    's3',
    aws_access_key_id='AKIA...',
    aws_secret_access_key='...'
)

# ‚ùå BAD: Even ENV vars are risky (need rotation)
s3_client = boto3.client('s3')  # Uses AWS_ACCESS_KEY_ID env var
```

### OCI (Good - Instance Principals)
```python
import oci

# ‚úÖ GOOD: Instance Principal (zero keys)
signer = oci.auth.signers.InstancePrincipalsSecurityTokenSigner()
object_storage_client = oci.object_storage.ObjectStorageClient(
    config={}, 
    signer=signer
)

# That's it! No keys, no secrets, no rotation.
# The VM's identity IS the credential.
```

### Setup Instance Principals (Terraform)
```hcl
# 1. Create Dynamic Group for Tundra VMs
resource "oci_identity_dynamic_group" "tundra_vms" {
  compartment_id = var.tenancy_ocid
  name           = "tundra-vms"
  description    = "All Tundra cluster VMs"
  
  # Match VMs by tag or compartment
  matching_rule = <<-EOT
    ALL {
      instance.compartment.id = '${var.compartment_ocid}',
      tag.domo.role.value = 'tundra'
    }
  EOT
}

# 2. Create Policy granting access to Object Storage
resource "oci_identity_policy" "tundra_object_storage" {
  compartment_id = var.tenancy_ocid
  name           = "tundra-object-storage-policy"
  description    = "Allow Tundra VMs to read Object Storage"
  
  statements = [
    "Allow dynamic-group tundra-vms to read objects in compartment ${var.compartment_name}",
    "Allow dynamic-group tundra-vms to read buckets in compartment ${var.compartment_name}",
    "Allow dynamic-group tundra-vms to read object-family in compartment ${var.compartment_name}",
    
    # For writes (ETL workloads)
    "Allow dynamic-group tundra-vms to manage objects in compartment ${var.compartment_name}",
    
    # For Vault (encryption keys)
    "Allow dynamic-group tundra-vms to use keys in compartment ${var.compartment_name}",
    "Allow dynamic-group tundra-vms to use key-delegates in compartment ${var.compartment_name}",
    
    # For OCI Cache
    "Allow dynamic-group tundra-vms to use redis-family in compartment ${var.compartment_name}",
    
    # For metrics/logs
    "Allow dynamic-group tundra-vms to use metrics in compartment ${var.compartment_name}",
    "Allow dynamic-group tundra-vms to use log-content in compartment ${var.compartment_name}",
  ]
}

# 3. Tag VMs so they match the dynamic group
resource "oci_core_instance" "tundra_node" {
  # ... other config ...
  
  defined_tags = {
    "domo.role" = "tundra"
  }
}
```

### Python Helper: Auto-detect OCI vs AWS
```python
import os
import oci
import boto3

def get_cloud_clients():
    """
    Auto-detect cloud provider and return appropriate clients.
    Use Instance Principals on OCI, fallback to boto3 on AWS.
    """
    # Check if running on OCI (instance metadata endpoint)
    try:
        import requests
        resp = requests.get(
            'http://169.254.169.254/opc/v2/instance/',
            timeout=1,
            headers={'Authorization': 'Bearer Oracle'}
        )
        if resp.status_code == 200:
            # We're on OCI - use Instance Principals
            print("‚úÖ Detected OCI - using Instance Principals (zero keys)")
            signer = oci.auth.signers.InstancePrincipalsSecurityTokenSigner()
            
            object_storage = oci.object_storage.ObjectStorageClient(
                config={}, signer=signer
            )
            namespace = object_storage.get_namespace().data
            
            return {
                'cloud': 'oci',
                'object_storage': object_storage,
                'namespace': namespace,
                'compute': oci.core.ComputeClient(config={}, signer=signer),
                'monitoring': oci.monitoring.MonitoringClient(config={}, signer=signer),
            }
    except:
        pass
    
    # Fallback to AWS
    print("‚ö†Ô∏è  Detected AWS - using boto3 (with access keys)")
    return {
        'cloud': 'aws',
        's3': boto3.client('s3'),
        'ec2': boto3.client('ec2'),
        'cloudwatch': boto3.client('cloudwatch'),
    }

# Usage in your code
clients = get_cloud_clients()

if clients['cloud'] == 'oci':
    # OCI code path
    bucket_name = 'domo-tenant-123-oci'
    objects = clients['object_storage'].list_objects(
        namespace_name=clients['namespace'],
        bucket_name=bucket_name
    ).data.objects
else:
    # AWS code path
    bucket_name = 'domo-tenant-123'
    objects = clients['s3'].list_objects_v2(
        Bucket=bucket_name
    )['Contents']
```

================================================================================
                    PART 3: OBJECT STORAGE PATTERNS
================================================================================

## AWS S3 ‚Üí OCI Object Storage Migration

### Pattern 1: Read/Write Operations
```python
import oci
from typing import BinaryIO

class ObjectStorageManager:
    """
    OCI Object Storage wrapper with Instance Principal auth.
    Drop-in replacement for boto3 S3 operations.
    """
    
    def __init__(self):
        # ‚úÖ Instance Principals - no keys!
        signer = oci.auth.signers.InstancePrincipalsSecurityTokenSigner()
        self.client = oci.object_storage.ObjectStorageClient(
            config={}, signer=signer
        )
        self.namespace = self.client.get_namespace().data
    
    def upload_file(self, bucket_name: str, object_name: str, 
                    file_path: str, metadata: dict = None):
        """
        Upload file to Object Storage.
        Equivalent to: s3.upload_file(file_path, bucket, key)
        """
        with open(file_path, 'rb') as f:
            self.client.put_object(
                namespace_name=self.namespace,
                bucket_name=bucket_name,
                object_name=object_name,
                put_object_body=f,
                metadata=metadata or {}
            )
    
    def download_file(self, bucket_name: str, object_name: str, 
                      file_path: str):
        """
        Download file from Object Storage.
        Equivalent to: s3.download_file(bucket, key, file_path)
        """
        response = self.client.get_object(
            namespace_name=self.namespace,
            bucket_name=bucket_name,
            object_name=object_name
        )
        
        with open(file_path, 'wb') as f:
            for chunk in response.data.raw.stream(1024 * 1024, decode_content=False):
                f.write(chunk)
    
    def list_objects(self, bucket_name: str, prefix: str = ''):
        """
        List objects with prefix.
        Equivalent to: s3.list_objects_v2(Bucket=bucket, Prefix=prefix)
        """
        # OCI paginates automatically
        objects = []
        next_start_with = None
        
        while True:
            response = self.client.list_objects(
                namespace_name=self.namespace,
                bucket_name=bucket_name,
                prefix=prefix,
                start=next_start_with,
                limit=1000  # Max per page
            )
            
            objects.extend(response.data.objects)
            
            next_start_with = response.data.next_start_with
            if not next_start_with:
                break
        
        return objects
    
    def delete_object(self, bucket_name: str, object_name: str):
        """Delete object. Equivalent to: s3.delete_object(Bucket, Key)"""
        self.client.delete_object(
            namespace_name=self.namespace,
            bucket_name=bucket_name,
            object_name=object_name
        )
    
    def get_object_metadata(self, bucket_name: str, object_name: str):
        """Get object metadata without downloading content."""
        response = self.client.head_object(
            namespace_name=self.namespace,
            bucket_name=bucket_name,
            object_name=object_name
        )
        return {
            'content_length': response.headers['Content-Length'],
            'content_type': response.headers['Content-Type'],
            'etag': response.headers['ETag'],
            'last_modified': response.headers['Last-Modified'],
            'metadata': response.headers.get('opc-meta-', {})
        }

# Usage
storage = ObjectStorageManager()

# Upload
storage.upload_file(
    bucket_name='domo-tenant-42-oci',
    object_name='raw/2025/01/15/data.parquet',
    file_path='/tmp/data.parquet',
    metadata={'tenant': '42', 'pipeline': 'nightly-etl'}
)

# Download
storage.download_file(
    bucket_name='domo-tenant-42-oci',
    object_name='raw/2025/01/15/data.parquet',
    file_path='/tmp/downloaded.parquet'
)

# List
objects = storage.list_objects(
    bucket_name='domo-tenant-42-oci',
    prefix='raw/2025/01/'
)
print(f"Found {len(objects)} objects")
```

### Pattern 2: Parallel Downloads for Hydration (FAST!)
```python
import oci
import concurrent.futures
import os

class FastHydrator:
    """
    Parallel downloader for Tundra cluster hydration.
    Leverages OCI's 100Gbps network for 3-8 minute hydration.
    """
    
    def __init__(self, max_workers: int = 32):
        signer = oci.auth.signers.InstancePrincipalsSecurityTokenSigner()
        self.client = oci.object_storage.ObjectStorageClient(
            config={}, signer=signer
        )
        self.namespace = self.client.get_namespace().data
        self.max_workers = max_workers
    
    def hydrate_from_object_storage(self, bucket_name: str, 
                                     prefix: str, local_dir: str):
        """
        Download all objects with prefix to local directory in parallel.
        Use for Tundra cluster hydration.
        
        Target: 100GB in 3-5 minutes with 32 parallel streams.
        """
        # List all objects
        print(f"üìã Listing objects in {bucket_name}/{prefix}...")
        objects = self._list_all_objects(bucket_name, prefix)
        print(f"‚úÖ Found {len(objects)} objects to download")
        
        # Download in parallel
        print(f"‚¨áÔ∏è  Downloading with {self.max_workers} parallel streams...")
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = []
            for obj in objects:
                local_path = os.path.join(local_dir, obj.name)
                future = executor.submit(
                    self._download_single_object,
                    bucket_name, obj.name, local_path
                )
                futures.append(future)
            
            # Wait for all downloads
            for i, future in enumerate(concurrent.futures.as_completed(futures)):
                try:
                    result = future.result()
                    if (i + 1) % 100 == 0:
                        print(f"‚úÖ Downloaded {i+1}/{len(objects)} objects")
                except Exception as e:
                    print(f"‚ùå Error: {e}")
        
        print(f"‚úÖ Hydration complete: {len(objects)} objects downloaded to {local_dir}")
    
    def _list_all_objects(self, bucket_name: str, prefix: str):
        objects = []
        next_start_with = None
        
        while True:
            response = self.client.list_objects(
                namespace_name=self.namespace,
                bucket_name=bucket_name,
                prefix=prefix,
                start=next_start_with,
                limit=1000
            )
            objects.extend(response.data.objects)
            next_start_with = response.data.next_start_with
            if not next_start_with:
                break
        
        return objects
    
    def _download_single_object(self, bucket_name: str, 
                                 object_name: str, local_path: str):
        """Download single object with retry logic."""
        os.makedirs(os.path.dirname(local_path), exist_ok=True)
        
        max_retries = 3
        for attempt in range(max_retries):
            try:
                response = self.client.get_object(
                    namespace_name=self.namespace,
                    bucket_name=bucket_name,
                    object_name=object_name
                )
                
                with open(local_path, 'wb') as f:
                    # Stream in 1MB chunks
                    for chunk in response.data.raw.stream(
                        1024 * 1024, decode_content=False
                    ):
                        f.write(chunk)
                
                return True
            except Exception as e:
                if attempt < max_retries - 1:
                    print(f"‚ö†Ô∏è  Retry {attempt+1} for {object_name}: {e}")
                    continue
                else:
                    raise

# Usage in Tundra cluster startup
hydrator = FastHydrator(max_workers=32)

import time
start = time.time()

hydrator.hydrate_from_object_storage(
    bucket_name='domo-tenant-42-oci',
    prefix='processed/',  # Only download processed data
    local_dir='/mnt/tundra-data'
)

elapsed = time.time() - start
print(f"‚è±Ô∏è  Hydration took {elapsed:.1f} seconds")
# Expected: 100GB in 180-480 seconds (3-8 minutes) on OCI
```

### Pattern 3: Pre-Authenticated Requests (Better than S3 pre-signed URLs)
```python
import oci
from datetime import datetime, timedelta

def create_par(bucket_name: str, object_name: str, 
               access_type: str = 'ObjectRead',
               time_expires: int = 3600):
    """
    Create Pre-Authenticated Request (PAR) for secure, time-limited access.
    Better than S3 pre-signed URLs: longer expiration, more granular control.
    
    access_type: 'ObjectRead', 'ObjectWrite', 'AnyObjectRead'
    time_expires: seconds until expiration (max: 315360000 = 10 years!)
    """
    signer = oci.auth.signers.InstancePrincipalsSecurityTokenSigner()
    client = oci.object_storage.ObjectStorageClient(config={}, signer=signer)
    namespace = client.get_namespace().data
    
    # Calculate expiration time
    expires = datetime.utcnow() + timedelta(seconds=time_expires)
    
    par_details = oci.object_storage.models.CreatePreauthenticatedRequestDetails(
        name=f"par-{object_name}-{int(datetime.now().timestamp())}",
        access_type=access_type,
        time_expires=expires,
        object_name=object_name  # Omit for bucket-level PAR
    )
    
    par = client.create_preauthenticated_request(
        namespace_name=namespace,
        bucket_name=bucket_name,
        create_preauthenticated_request_details=par_details
    )
    
    # Construct full URL
    region = 'us-ashburn-1'  # Get from config
    full_url = f"https://objectstorage.{region}.oraclecloud.com{par.data.access_uri}"
    
    return {
        'url': full_url,
        'expires_at': par.data.time_expires,
        'access_type': par.data.access_type
    }

# Usage: Share dataset with customer
par = create_par(
    bucket_name='domo-exports',
    object_name='customer-42/report-2025-01.csv',
    access_type='ObjectRead',
    time_expires=86400  # 24 hours
)

print(f"Share this URL: {par['url']}")
print(f"Expires at: {par['expires_at']}")
# Customer can download without authentication for 24 hours
```

================================================================================
                        PART 4: COMPUTE PATTERNS
================================================================================

## OCI Flex Shapes: The Game Changer

**Key Advantage: Resize CPU/RAM without reboot!**

### Terraform: Tundra Cluster with Flex Shapes
```hcl
# Tundra cluster VM with Flex shape
resource "oci_core_instance" "tundra_node" {
  availability_domain = data.oci_identity_availability_domains.ads.availability_domains[0].name
  compartment_id      = var.compartment_ocid
  display_name        = "tundra-node-${var.tenant_id}-${count.index}"
  
  # ‚úÖ FLEX SHAPE: Can resize dynamically
  shape = "VM.Standard.E5.Flex"
  
  shape_config {
    # Start with 16 OCPUs, 256GB RAM
    ocpus         = 16
    memory_in_gbs = 256
    
    # Can later resize to 8-64 OCPUs, 128GB-1TB RAM
    # WITHOUT rebooting or re-hydrating!
  }
  
  source_details {
    source_type = "image"
    source_id   = var.tundra_image_ocid
    
    # Boot volume size
    boot_volume_size_in_gbs = 100
  }
  
  create_vnic_details {
    subnet_id        = var.private_subnet_ocid
    assign_public_ip = false
    
    # High-performance networking
    hostname_label = "tundra-node-${var.tenant_id}-${count.index}"
  }
  
  metadata = {
    ssh_authorized_keys = var.ssh_public_key
    user_data           = base64encode(templatefile("${path.module}/cloud-init.yaml", {
      tenant_id    = var.tenant_id
      bucket_name  = var.bucket_name
      cluster_role = count.index == 0 ? "master" : "worker"
    }))
  }
  
  # ‚úÖ CRITICAL: Tag for Instance Principal
  defined_tags = {
    "domo.role"      = "tundra"
    "domo.tenant_id" = var.tenant_id
  }
  
  # Placement: Spread across ADs for HA
  count = 3
}

# ‚úÖ ULTRA HIGH PERFORMANCE Block Volume (auto-tuned)
resource "oci_core_volume" "tundra_data" {
  availability_domain = data.oci_identity_availability_domains.ads.availability_domains[0].name
  compartment_id      = var.compartment_ocid
  display_name        = "tundra-data-${var.tenant_id}-${count.index}"
  
  size_in_gbs = 1000  # 1TB
  
  # ‚úÖ AUTO-TUNED: OCI automatically optimizes IOPS/throughput
  vpus_per_gb = 20  # Ultra High Performance (200K IOPS)
  
  # Auto-tune enabled (default)
  auto_tune_performance_enabled = true
  
  count = 3
}

resource "oci_core_volume_attachment" "tundra_data_attach" {
  instance_id     = oci_core_instance.tundra_node[count.index].id
  volume_id       = oci_core_volume.tundra_data[count.index].id
  attachment_type = "paravirtualized"
  
  count = 3
}
```

### Python: Dynamic Flex Shape Resizing (No Reboot!)
```python
import oci

def resize_tundra_cluster(instance_id: str, new_ocpus: int, 
                          new_memory_gb: int):
    """
    Resize Flex shape WITHOUT reboot.
    
    Use cases:
    - Scale up for peak hours (8am: 32 OCPUs)
    - Scale down for off-peak (6pm: 16 OCPUs)
    - Emergency: Scale up if CPU >80%
    
    Time: 3-5 minutes, NO downtime, data stays in memory!
    """
    signer = oci.auth.signers.InstancePrincipalsSecurityTokenSigner()
    compute_client = oci.core.ComputeClient(config={}, signer=signer)
    
    # Get current instance details
    instance = compute_client.get_instance(instance_id).data
    print(f"Current: {instance.shape_config.ocpus} OCPUs, "
          f"{instance.shape_config.memory_in_gbs}GB RAM")
    
    # Update shape config (no reboot!)
    update_details = oci.core.models.UpdateInstanceDetails(
        shape_config=oci.core.models.UpdateInstanceShapeConfigDetails(
            ocpus=new_ocpus,
            memory_in_gbs=new_memory_gb
        )
    )
    
    print(f"Resizing to: {new_ocpus} OCPUs, {new_memory_gb}GB RAM...")
    print("‚è≥ This takes 3-5 minutes (no reboot, no data loss)")
    
    compute_client.update_instance(
        instance_id=instance_id,
        update_instance_details=update_details
    )
    
    print("‚úÖ Resize initiated. Cluster remains operational during resize.")

# Usage: Scale up for business hours
resize_tundra_cluster(
    instance_id='ocid1.instance.oc1.iad.xxxxx',
    new_ocpus=32,
    new_memory_gb=512
)

# Later: Scale down for off-peak
resize_tundra_cluster(
    instance_id='ocid1.instance.oc1.iad.xxxxx',
    new_ocpus=16,
    new_memory_gb=256
)

# üí∞ SAVINGS: 50% cost reduction during off-peak hours
# ‚ö° ADVANTAGE: On AWS, would need to terminate & re-hydrate (10-15 min downtime)
```

### Pattern: Auto-Scaling Based on Query Load
```python
import oci
from datetime import datetime, timedelta

class TundraAutoScaler:
    """
    Auto-scale Tundra clusters based on query load.
    Uses OCI Monitoring metrics to trigger Flex shape resizing.
    """
    
    def __init__(self):
        signer = oci.auth.signers.InstancePrincipalsSecurityTokenSigner()
        self.compute = oci.core.ComputeClient(config={}, signer=signer)
        self.monitoring = oci.monitoring.MonitoringClient(config={}, signer=signer)
        self.compartment_id = self._get_compartment_id()
    
    def check_and_scale(self, instance_id: str):
        """
        Check CPU/memory metrics and scale if needed.
        Run this every 5 minutes via cron or OCI Functions.
        """
        # Get current metrics
        cpu_avg = self._get_metric(instance_id, 'CpuUtilization', minutes=5)
        memory_avg = self._get_metric(instance_id, 'MemoryUtilization', minutes=5)
        
        # Get current shape config
        instance = self.compute.get_instance(instance_id).data
        current_ocpus = instance.shape_config.ocpus
        current_memory = instance.shape_config.memory_in_gbs
        
        print(f"üìä Current: {current_ocpus} OCPUs, {current_memory}GB RAM")
        print(f"üìà Metrics: CPU {cpu_avg:.1f}%, Memory {memory_avg:.1f}%")
        
        # Scale up if high utilization
        if cpu_avg > 75 or memory_avg > 75:
            new_ocpus = min(current_ocpus * 2, 64)  # Max 64
            new_memory = min(current_memory * 2, 1024)  # Max 1TB
            
            print(f"üîº SCALING UP to {new_ocpus} OCPUs, {new_memory}GB RAM")
            self._resize(instance_id, new_ocpus, new_memory)
        
        # Scale down if low utilization
        elif cpu_avg < 30 and memory_avg < 30:
            new_ocpus = max(current_ocpus // 2, 8)  # Min 8
            new_memory = max(current_memory // 2, 128)  # Min 128GB
            
            print(f"üîΩ SCALING DOWN to {new_ocpus} OCPUs, {new_memory}GB RAM")
            self._resize(instance_id, new_ocpus, new_memory)
        
        else:
            print("‚úÖ No scaling needed")
    
    def _get_metric(self, instance_id: str, metric_name: str, 
                    minutes: int = 5) -> float:
        """Get average metric value for last N minutes."""
        end_time = datetime.utcnow()
        start_time = end_time - timedelta(minutes=minutes)
        
        query = f'CpuUtilization[{minutes}m].mean()'
        
        # OCI Monitoring query
        response = self.monitoring.summarize_metrics_data(
            compartment_id=self.compartment_id,
            summarize_metrics_data_details=oci.monitoring.models.SummarizeMetricsDataDetails(
                namespace='oci_computeagent',
                query=query,
                start_time=start_time,
                end_time=end_time
            )
        )
        
        if response.data:
            return response.data[0].aggregated_datapoints[0].value
        return 0.0
    
    def _resize(self, instance_id: str, ocpus: int, memory_gb: int):
        """Resize instance (Flex shape - no reboot)."""
        self.compute.update_instance(
            instance_id=instance_id,
            update_instance_details=oci.core.models.UpdateInstanceDetails(
                shape_config=oci.core.models.UpdateInstanceShapeConfigDetails(
                    ocpus=ocpus,
                    memory_in_gbs=memory_gb
                )
            )
        )

# Deploy as OCI Function (runs every 5 minutes)
def handler(ctx, data: dict):
    scaler = TundraAutoScaler()
    
    # Check all Tundra instances
    for instance_id in data.get('instance_ids', []):
        scaler.check_and_scale(instance_id)
    
    return {'status': 'success'}
```

================================================================================
                      PART 5: MIGRATION-SPECIFIC PATTERNS
================================================================================

## DNS Cutover (Zero Downtime)
```python
import time
import boto3
from typing import List

class ZeroDowntimeCutover:
    """
    Orchestrate DNS cutover from AWS to OCI with zero downtime.
    """
    
    def __init__(self, hosted_zone_id: str):
        self.route53 = boto3.client('route53')
        self.hosted_zone_id = hosted_zone_id
    
    def cutover_tenant(self, tenant_domain: str, 
                       aws_lb_ip: str, 
                       oci_lb_ip: str,
                       ttl: int = 60):
        """
        Cutover tenant from AWS to OCI.
        
        Steps:
        1. Lower TTL to 60s (if not already)
        2. Wait 5 minutes for DNS caches to refresh
        3. Update A record to OCI IP
        4. Monitor traffic shift
        
        Total time: ~7 minutes
        Downtime: 0 seconds (gradual shift)
        """
        print(f"üîÑ Starting cutover for {tenant_domain}")
        print(f"   AWS: {aws_lb_ip} ‚Üí OCI: {oci_lb_ip}")
        
        # Step 1: Ensure TTL is 60s
        current_record = self._get_current_record(tenant_domain)
        if current_record['TTL'] != ttl:
            print(f"‚è±Ô∏è  Lowering TTL to {ttl}s...")
            self._update_dns(tenant_domain, aws_lb_ip, ttl)
            print(f"‚è≥ Waiting 5 minutes for DNS caches to refresh...")
            time.sleep(300)
        
        # Step 2: Update to OCI IP
        print(f"üîÑ Updating DNS to OCI: {oci_lb_ip}")
        self._update_dns(tenant_domain, oci_lb_ip, ttl)
        
        # Step 3: Monitor traffic shift
        print(f"üìä Monitoring traffic shift (takes 1-2 minutes)...")
        self._monitor_traffic_shift(tenant_domain, oci_lb_ip)
        
        print(f"‚úÖ Cutover complete for {tenant_domain}")
        print(f"   All traffic now on OCI: {oci_lb_ip}")
    
    def _update_dns(self, domain: str, ip: str, ttl: int):
        """Update Route53 A record."""
        self.route53.change_resource_record_sets(
            HostedZoneId=self.hosted_zone_id,
            ChangeBatch={
                'Changes': [{
                    'Action': 'UPSERT',
                    'ResourceRecordSet': {
                        'Name': domain,
                        'Type': 'A',
                        'TTL': ttl,
                        'ResourceRecords': [{'Value': ip}]
                    }
                }]
            }
        )
    
    def _get_current_record(self, domain: str) -> dict:
        """Get current DNS record."""
        response = self.route53.list_resource_record_sets(
            HostedZoneId=self.hosted_zone_id,
            StartRecordName=domain,
            StartRecordType='A',
            MaxItems='1'
        )
        
        for record in response['ResourceRecordSets']:
            if record['Name'] == domain + '.':
                return {
                    'IP': record['ResourceRecords'][0]['Value'],
                    'TTL': record['TTL']
                }
        
        raise ValueError(f"Record not found: {domain}")
    
    def _monitor_traffic_shift(self, domain: str, oci_ip: str):
        """
        Monitor traffic shift by checking access logs.
        Expect gradual shift over 1-2 minutes.
        """
        for i in range(12):  # Check every 10 seconds for 2 minutes
            time.sleep(10)
            
            # Query OCI Load Balancer metrics
            # (Simplified - would use OCI Monitoring API in production)
            print(f"   Minute {(i+1)/6:.1f}: Traffic shifting...")
        
        print(f"   ‚úÖ Traffic fully shifted to OCI")

# Usage
cutover = ZeroDowntimeCutover(hosted_zone_id='Z1234567890ABC')

cutover.cutover_tenant(
    tenant_domain='tenant42.domo.com',
    aws_lb_ip='52.1.2.3',
    oci_lb_ip='129.213.45.67',
    ttl=60
)
```

## Rollback Procedure
```python
import oci
import boto3
import time

class EmergencyRollback:
    """
    Fast rollback from OCI to AWS if critical issues detected.
    Target: <5 minutes to restore service.
    """
    
    def __init__(self, hosted_zone_id: str):
        self.route53 = boto3.client('route53')
        self.hosted_zone_id = hosted_zone_id
        
        # OCI monitoring for health checks
        signer = oci.auth.signers.InstancePrincipalsSecurityTokenSigner()
        self.monitoring = oci.monitoring.MonitoringClient(config={}, signer=signer)
    
    def rollback_tenant(self, tenant_id: str, tenant_domain: str, 
                        aws_lb_ip: str, reason: str):
        """
        Emergency rollback from OCI to AWS.
        
        Triggers:
        - Error rate >5% for >5 minutes
        - Query latency >3x baseline
        - OCI cluster unhealthy
        - Customer escalation
        """
        print(f"üö® EMERGENCY ROLLBACK for {tenant_id}")
        print(f"   Reason: {reason}")
        print(f"   Reverting DNS to AWS: {aws_lb_ip}")
        
        # Step 1: Revert DNS (fastest path to recovery)
        self._revert_dns(tenant_domain, aws_lb_ip)
        
        # Step 2: Verify AWS cluster is healthy
        if not self._check_aws_cluster_health(tenant_id):
            print(f"‚ö†Ô∏è  WARNING: AWS cluster may not be healthy!")
            print(f"   Manual intervention required")
        
        # Step 3: Monitor traffic shift back to AWS
        print(f"‚è≥ Waiting for traffic to shift back to AWS...")
        time.sleep(120)  # 2 minutes for DNS propagation
        
        # Step 4: Notify team
        self._send_alerts(tenant_id, reason)
        
        print(f"‚úÖ Rollback complete for {tenant_id}")
        print(f"   Tenant now back on AWS: {aws_lb_ip}")
        print(f"   Investigate OCI issue: {reason}")
    
    def _revert_dns(self, domain: str, aws_ip: str):
        """Revert DNS to AWS."""
        self.route53.change_resource_record_sets(
            HostedZoneId=self.hosted_zone_id,
            ChangeBatch={
                'Changes': [{
                    'Action': 'UPSERT',
                    'ResourceRecordSet': {
                        'Name': domain,
                        'Type': 'A',
                        'TTL': 60,  # Keep low TTL for fast changes
                        'ResourceRecords': [{'Value': aws_ip}]
                    }
                }]
            }
        )
        print(f"‚úÖ DNS reverted to AWS: {aws_ip}")
    
    def _check_aws_cluster_health(self, tenant_id: str) -> bool:
        """
        Verify AWS Tundra cluster is still healthy.
        (We kept it running for 24h post-migration!)
        """
        # In production: Check EC2 instance status, run test query
        # Simplified here
        return True
    
    def _send_alerts(self, tenant_id: str, reason: str):
        """Send PagerDuty/Slack alerts."""
        # Slack
        import requests
        webhook_url = 'https://hooks.slack.com/services/...'
        requests.post(webhook_url, json={
            'text': f'üö® ROLLBACK: Tenant {tenant_id} rolled back to AWS. Reason: {reason}'
        })
        
        # PagerDuty
        # ... trigger incident
        
        print(f"üì¢ Alerts sent to team")

# Usage: Automated rollback based on metrics
def check_tenant_health_and_rollback(tenant_id: str):
    """
    Check tenant health post-migration.
    Auto-rollback if critical issues detected.
    """
    signer = oci.auth.signers.InstancePrincipalsSecurityTokenSigner()
    monitoring = oci.monitoring.MonitoringClient(config={}, signer=signer)
    
    # Get error rate (last 5 minutes)
    error_rate = get_error_rate(tenant_id, monitoring)
    
    if error_rate > 5.0:  # >5% error rate
        print(f"üö® ERROR RATE TOO HIGH: {error_rate:.1f}%")
        
        rollback = EmergencyRollback(hosted_zone_id='Z1234567890ABC')
        rollback.rollback_tenant(
            tenant_id=tenant_id,
            tenant_domain=f'tenant{tenant_id}.domo.com',
            aws_lb_ip='52.1.2.3',
            reason=f'Error rate {error_rate:.1f}% (threshold: 5%)'
        )
```

================================================================================
                        PART 6: COST OPTIMIZATION PATTERNS
================================================================================

## Free Egress Optimization
```python
"""
OCI provides 10TB/month FREE egress (vs AWS $922/TB after 100GB).
Optimize data access patterns to maximize this benefit.
"""

class CostOptimizedDataAccess:
    """
    Optimize data access to stay within free egress tier.
    Track usage and alert when approaching limit.
    """
    
    def __init__(self):
        signer = oci.auth.signers.InstancePrincipalsSecurityTokenSigner()
        self.monitoring = oci.monitoring.MonitoringClient(config={}, signer=signer)
        self.object_storage = oci.object_storage.ObjectStorageClient(
            config={}, signer=signer
        )
        self.namespace = self.object_storage.get_namespace().data
    
    def get_monthly_egress(self, compartment_id: str) -> float:
        """
        Get total egress from Object Storage this month (GB).
        """
        # Query OCI Monitoring for egress metrics
        # (Simplified - real implementation would use Monitoring API)
        
        # For now, estimate from bucket sizes and download patterns
        total_egress_gb = 0.0
        
        # List all buckets
        buckets = self.object_storage.list_buckets(
            namespace_name=self.namespace,
            compartment_id=compartment_id
        ).data
        
        for bucket in buckets:
            # Get bucket statistics
            stats = self.object_storage.get_bucket(
                namespace_name=self.namespace,
                bucket_name=bucket.name
            ).data.approximate_size
            
            # Estimate: Assume each bucket downloaded 2x this month
            # (In prod: Track actual downloads via access logs)
            estimated_egress_gb = (stats / (1024**3)) * 2
            total_egress_gb += estimated_egress_gb
        
        return total_egress_gb
    
    def check_and_alert(self, compartment_id: str):
        """
        Check egress usage and alert if approaching 10TB limit.
        """
        egress_gb = self.get_monthly_egress(compartment_id)
        limit_gb = 10 * 1024  # 10TB = 10,240 GB
        
        usage_pct = (egress_gb / limit_gb) * 100
        
        print(f"üìä Object Storage Egress This Month:")
        print(f"   Used: {egress_gb:.1f} GB / {limit_gb} GB ({usage_pct:.1f}%)")
        print(f"   Cost: $0 (within free tier)")
        
        if usage_pct > 80:
            print(f"‚ö†Ô∏è  WARNING: Approaching free egress limit!")
            print(f"   Consider optimizing data access patterns")
            self._send_alert(egress_gb, limit_gb)
        
        elif usage_pct > 100:
            overage_gb = egress_gb - limit_gb
            overage_cost = overage_gb * 0.0085  # $0.0085/GB after 10TB
            print(f"üí∞ OVERAGE: {overage_gb:.1f} GB beyond free tier")
            print(f"   Extra cost: ${overage_cost:.2f}")
    
    def _send_alert(self, used_gb: float, limit_gb: float):
        """Send alert to team about egress usage."""
        # Slack notification
        import requests
        webhook = 'https://hooks.slack.com/services/...'
        requests.post(webhook, json={
            'text': f'‚ö†Ô∏è  OCI Egress: {used_gb:.0f}GB / {limit_gb}GB used (80%+)'
        })

# Run monthly
cost_optimizer = CostOptimizedDataAccess()
cost_optimizer.check_and_alert(compartment_id='ocid1.compartment...')
```

## Auto-Tiering for Storage Cost Savings
```python
import oci

def enable_auto_tiering(bucket_name: str):
    """
    Enable OCI Object Storage Auto-Tiering.
    
    Benefits:
    - Automatically moves infrequently accessed data to cheaper tiers
    - Standard ‚Üí Infrequent Access ‚Üí Archive
    - 30-60% cost savings on storage
    - FREE feature (no lifecycle policy management needed like AWS)
    """
    signer = oci.auth.signers.InstancePrincipalsSecurityTokenSigner()
    client = oci.object_storage.ObjectStorageClient(config={}, signer=signer)
    namespace = client.get_namespace().data
    
    # Update bucket to enable auto-tiering
    update_details = oci.object_storage.models.UpdateBucketDetails(
        auto_tiering='InfrequentAccess'  # or 'Disabled'
    )
    
    client.update_bucket(
        namespace_name=namespace,
        bucket_name=bucket_name,
        update_bucket_details=update_details
    )
    
    print(f"‚úÖ Auto-tiering enabled for {bucket_name}")
    print(f"   OCI will automatically move cold data to cheaper tiers")
    print(f"   Expected savings: 30-60% on storage costs")

# Enable for all tenant buckets
for tenant_id in range(1, 1001):
    enable_auto_tiering(f'domo-tenant-{tenant_id}-oci')
```

================================================================================
                      PART 7: COMMON GOTCHAS & SOLUTIONS
================================================================================

## Gotcha 1: OCI Region Naming
```python
# ‚ùå WRONG: AWS-style region names don't work
region = 'us-east-1'  # AWS style

# ‚úÖ CORRECT: OCI region names are different
OCI_REGIONS = {
    'us-east-1': 'us-ashburn-1',    # Virginia ‚Üí Ashburn
    'us-west-1': 'us-phoenix-1',    # N. California ‚Üí Phoenix
    'us-west-2': 'us-sanjose-1',    # Oregon ‚Üí San Jose
    'eu-west-1': 'eu-frankfurt-1',  # Ireland ‚Üí Frankfurt
    'ap-southeast-1': 'ap-tokyo-1', # Singapore ‚Üí Tokyo
}

# Use this helper
def aws_to_oci_region(aws_region: str) -> str:
    """Convert AWS region to OCI region."""
    mapping = {
        'us-east-1': 'us-ashburn-1',
        'us-west-2': 'us-phoenix-1',
        'eu-west-1': 'eu-frankfurt-1',
    }
    return mapping.get(aws_region, 'us-ashburn-1')
```

## Gotcha 2: OCI Namespace is Required
```python
# ‚ùå WRONG: Forgetting namespace (doesn't exist in S3)
client.list_objects(bucket_name='my-bucket')  # ERROR!

# ‚úÖ CORRECT: Always include namespace
namespace = client.get_namespace().data  # Get once, cache it
client.list_objects(
    namespace_name=namespace,  # Required!
    bucket_name='my-bucket'
)
```

## Gotcha 3: OCI Pagination
```python
# ‚ùå WRONG: Assuming all results returned (like boto3 with max 1000)
response = client.list_objects(namespace, bucket)
objects = response.data.objects  # May be incomplete!

# ‚úÖ CORRECT: Always paginate
def list_all_objects(client, namespace, bucket, prefix=''):
    """List ALL objects (handle pagination)."""
    all_objects = []
    next_start_with = None
    
    while True:
        response = client.list_objects(
            namespace_name=namespace,
            bucket_name=bucket,
            prefix=prefix,
            start=next_start_with,  # Pagination cursor
            limit=1000
        )
        
        all_objects.extend(response.data.objects)
        
        next_start_with = response.data.next_start_with
        if not next_start_with:  # No more pages
            break
    
    return all_objects
```

## Gotcha 4: OCI Limits (Different from AWS)
```python
"""
OCI has different limits than AWS. Check these before migration:

| Resource                    | AWS Limit  | OCI Limit   | Action Needed     |
|-----------------------------|------------|-------------|-------------------|
| VMs per region              | Varies     | 300 default | Request increase  |
| Block Volumes per VM        | 40         | 32          | OK for most       |
| VCNs per compartment        | N/A        | 5 default   | Use 1 VCN         |
| Subnets per VCN             | 200        | 300         | OK                |
| Security Rules per NSG      | 60         | 120         | Better on OCI!    |
| Load Balancer listeners     | 50         | 512         | Much better!      |

Request limit increases BEFORE migration:
- Service Limits page in OCI Console
- Typical approval: 1-3 business days
"""

def check_oci_limits():
    """Check OCI limits before migration."""
    signer = oci.auth.signers.InstancePrincipalsSecurityTokenSigner()
    limits_client = oci.limits.LimitsClient(config={}, signer=signer)
    
    # Check compute limits
    service_name = 'compute'
    limit_name = 'standard-e5-core-count'  # E5 Flex shape OCPUs
    
    response = limits_client.get_resource_availability(
        service_name=service_name,
        limit_name=limit_name,
        compartment_id='ocid1.compartment...',
        availability_domain='AD-1'
    )
    
    print(f"OCPUs Available: {response.data.available}")
    print(f"OCPUs Used: {response.data.used}")
    
    if response.data.available < 500:  # Need 500 OCPUs for Tundra clusters
        print("‚ö†Ô∏è  WARNING: Request OCPU limit increase!")
```

================================================================================
                      PART 8: TESTING & VALIDATION
================================================================================

## Automated Test Suite for Migration
```python
import oci
import time
import hashlib
from typing import Tuple

class MigrationValidator:
    """
    Comprehensive validation suite for AWS‚ÜíOCI migration.
    Run before DNS cutover to ensure OCI environment ready.
    """
    
    def __init__(self, tenant_id: str):
        self.tenant_id = tenant_id
        signer = oci.auth.signers.InstancePrincipalsSecurityTokenSigner()
        self.object_storage = oci.object_storage.ObjectStorageClient(
            config={}, signer=signer
        )
        self.compute = oci.core.ComputeClient(config={}, signer=signer)
        self.monitoring = oci.monitoring.MonitoringClient(config={}, signer=signer)
    
    def run_all_tests(self) -> Tuple[bool, list]:
        """
        Run complete test suite.
        Returns: (success, failed_tests)
        """
        tests = [
            ('Data Integrity', self.test_data_integrity),
            ('Query Performance', self.test_query_performance),
            ('Cluster Health', self.test_cluster_health),
            ('Network Connectivity', self.test_network),
            ('Failover', self.test_failover),
            ('E2E', self.test_end_to_end),
        ]
        
        results = []
        failed = []
        
        print(f"üß™ Running migration validation for tenant {self.tenant_id}")
        print("=" * 60)
        
        for test_name, test_func in tests:
            print(f"\n‚ñ∂Ô∏è  Test: {test_name}")
            try:
                result = test_func()
                if result:
                    print(f"   ‚úÖ PASS")
                    results.append((test_name, True))
                else:
                    print(f"   ‚ùå FAIL")
                    results.append((test_name, False))
                    failed.append(test_name)
            except Exception as e:
                print(f"   ‚ùå ERROR: {e}")
                results.append((test_name, False))
                failed.append(test_name)
        
        print("\n" + "=" * 60)
        print(f"üìä Results: {len(tests) - len(failed)}/{len(tests)} tests passed")
        
        if not failed:
            print("‚úÖ ALL TESTS PASSED - Ready for cutover")
            return True, []
        else:
            print(f"‚ùå {len(failed)} test(s) failed: {', '.join(failed)}")
            print("‚ö†Ô∏è  DO NOT proceed with cutover until issues resolved")
            return False, failed
    
    def test_data_integrity(self) -> bool:
        """
        Test 1: Verify data synced correctly from S3 to Object Storage.
        - Compare object counts
        - Sample checksum validation
        """
        # Get AWS S3 count (via API or database)
        aws_object_count = 1234  # Placeholder
        
        # Get OCI Object Storage count
        namespace = self.object_storage.get_namespace().data
        bucket = f'domo-tenant-{self.tenant_id}-oci'
        
        oci_objects = self._list_all_objects(namespace, bucket)
        oci_object_count = len(oci_objects)
        
        print(f"   AWS S3: {aws_object_count} objects")
        print(f"   OCI Object Storage: {oci_object_count} objects")
        
        # Allow 0.1% variance (some objects may be in-flight)
        variance = abs(aws_object_count - oci_object_count) / aws_object_count
        
        if variance > 0.001:  # >0.1% difference
            print(f"   ‚ö†Ô∏è  Object count mismatch: {variance*100:.2f}%")
            return False
        
        # Sample checksum validation (check 100 random objects)
        print(f"   Validating checksums for 100 sample objects...")
        # ... checksum comparison logic ...
        
        return True
    
    def test_query_performance(self) -> bool:
        """
        Test 2: Run sample queries and compare latency to AWS baseline.
        Target: OCI ‚â§ AWS latency (ideally 20-40% faster)
        """
        # Run 10 test queries
        test_queries = [
            "SELECT COUNT(*) FROM table1",
            "SELECT * FROM table2 WHERE date >= '2025-01-01' LIMIT 1000",
            # ... more queries
        ]
        
        latencies = []
        for query in test_queries:
            start = time.time()
            # Execute query against OCI Tundra cluster
            # ... query execution ...
            elapsed = time.time() - start
            latencies.append(elapsed)
        
        avg_latency = sum(latencies) / len(latencies)
        p95_latency = sorted(latencies)[int(len(latencies) * 0.95)]
        
        print(f"   Avg latency: {avg_latency*1000:.0f}ms")
        print(f"   P95 latency: {p95_latency*1000:.0f}ms")
        
        # Compare to AWS baseline
        aws_p95_baseline = 1200  # ms
        
        if p95_latency * 1000 > aws_p95_baseline:
            print(f"   ‚ö†Ô∏è  Slower than AWS baseline ({aws_p95_baseline}ms)")
            return False
        else:
            improvement = (1 - p95_latency*1000 / aws_p95_baseline) * 100
            print(f"   ‚úÖ {improvement:.1f}% faster than AWS!")
        
        return True
    
    def test_cluster_health(self) -> bool:
        """
        Test 3: Verify OCI Tundra cluster is healthy.
        - All nodes running
        - Memory < 80%
        - CPU < 70%
        - Disk < 80%
        """
        # Get cluster instances
        # ... query OCI Compute API ...
        
        healthy_nodes = 3  # Placeholder
        total_nodes = 3
        
        if healthy_nodes != total_nodes:
            print(f"   ‚ö†Ô∏è  Only {healthy_nodes}/{total_nodes} nodes healthy")
            return False
        
        print(f"   ‚úÖ {healthy_nodes}/{total_nodes} nodes healthy")
        return True
    
    def test_network(self) -> bool:
        """Test 4: Verify network connectivity."""
        # Test Tundra cluster ‚Üí Object Storage
        # Test Tundra cluster ‚Üí OCI Cache
        # Test Load Balancer ‚Üí Tundra cluster
        return True
    
    def test_failover(self) -> bool:
        """Test 5: Simulate node failure, verify failover."""
        # Terminate one Tundra node
        # Verify Load Balancer routes to healthy nodes
        # Verify query latency impact < 10%
        return True
    
    def test_end_to_end(self) -> bool:
        """Test 6: Full end-to-end test (login, load dashboard, query)."""
        # HTTP request to Load Balancer
        # Login as test user
        # Load dashboard
        # Execute query
        # Verify response < 2s
        return True
    
    def _list_all_objects(self, namespace, bucket):
        """Helper: List all objects in bucket."""
        objects = []
        next_start = None
        while True:
            resp = self.object_storage.list_objects(
                namespace_name=namespace,
                bucket_name=bucket,
                start=next_start,
                limit=1000
            )
            objects.extend(resp.data.objects)
            next_start = resp.data.next_start_with
            if not next_start:
                break
        return objects

# Usage: Run before each tenant cutover
validator = MigrationValidator(tenant_id='42')
success, failed_tests = validator.run_all_tests()

if success:
    print("\nüöÄ READY FOR CUTOVER")
    # Proceed with DNS cutover
else:
    print(f"\nüõë NOT READY - Fix these tests: {failed_tests}")
    # Do NOT cutover, investigate issues
```

================================================================================
                      PART 9: PROMPTS FOR AI ASSISTANTS
================================================================================

## When Asking AI to Generate OCI Code

**ALWAYS include these instructions in your prompt:**
```
Context: I'm working on migrating Domo from AWS to OCI.

Requirements:
1. Use OCI Instance Principals (zero-key auth) - NEVER use API keys
2. Use OCI SDK for Python: `import oci`
3. Always include error handling and retries
4. Use OCI Flex shapes where possible (dynamic sizing)
5. Optimize for OCI's free 10TB/month egress
6. Include logging and monitoring
7. Follow OCI naming conventions (us-ashburn-1, not us-east-1)
8. Always paginate OCI API responses
9. Use OCI Service Gateway for Object Storage (free access)
10. Include Terraform for infrastructure

Please generate code for: [YOUR REQUEST]
```

## Example Prompts

### Prompt 1: Create Tundra Cluster
```
Context: Domo AWS‚ÜíOCI migration. I need to create a Tundra cluster.

Requirements:
- Use Flex shapes (start 16 OCPUs, 256GB RAM)
- 3 nodes across different Availability Domains (HA)
- Instance Principals for Object Storage access
- Auto-tuned Block Volumes (Ultra High Performance)
- Security: Private subnet only
- Tags: domo.role=tundra, domo.tenant_id=42
- Include cloud-init script for Tundra installation

Generate Terraform code to create this cluster.
```

### Prompt 2: Data Sync Script
```
Context: Domo AWS‚ÜíOCI migration. I need to sync tenant data from S3 to Object Storage.

Requirements:
- Parallel downloads (32 workers) for speed
- Checksum validation (MD5)
- Progress logging
- Handle network errors with retry (3 attempts)
- Use Instance Principals (no keys)
- Optimize for OCI's 100Gbps network

Generate Python script to:
1. List all objects in AWS S3 bucket
2. Download to local temp dir
3. Upload to OCI Object Storage
4. Verify checksums match
```

### Prompt 3: Auto-Scaling
```
Context: Domo Tundra clusters on OCI.

Requirements:
- Monitor CPU and memory metrics (OCI Monitoring)
- Scale up if CPU >75% or Memory >75%
- Scale down if CPU <30% and Memory <30%
- Use Flex shapes (resize without reboot!)
- Min: 8 OCPUs, 128GB RAM
- Max: 64 OCPUs, 1TB RAM
- Run every 5 minutes (OCI Function)

Generate Python code for auto-scaler that resizes Flex shapes dynamically.
```

================================================================================
                      PART 10: CHEAT SHEET
================================================================================

## Quick Reference

### Common OCI SDK Patterns
```python
# Get Instance Principal signer (use everywhere)
signer = oci.auth.signers.InstancePrincipalsSecurityTokenSigner()

# Object Storage client
os_client = oci.object_storage.ObjectStorageClient(config={}, signer=signer)
namespace = os_client.get_namespace().data

# Upload file
with open('file.txt', 'rb') as f:
    os_client.put_object(namespace, 'bucket', 'object-name', f)

# Download file
response = os_client.get_object(namespace, 'bucket', 'object-name')
with open('file.txt', 'wb') as f:
    for chunk in response.data.raw.stream(1024*1024):
        f.write(chunk)

# List objects (with pagination)
next_start = None
while True:
    resp = os_client.list_objects(namespace, 'bucket', start=next_start)
    objects = resp.data.objects
    next_start = resp.data.next_start_with
    if not next_start:
        break

# Compute client
compute = oci.core.ComputeClient(config={}, signer=signer)

# Resize Flex shape (no reboot!)
compute.update_instance(
    instance_id='ocid1.instance...',
    update_instance_details=oci.core.models.UpdateInstanceDetails(
        shape_config=oci.core.models.UpdateInstanceShapeConfigDetails(
            ocpus=32,
            memory_in_gbs=512
        )
    )
)

# Monitoring client
monitoring = oci.monitoring.MonitoringClient(config={}, signer=signer)

# Query metrics
metrics = monitoring.summarize_metrics_data(
    compartment_id='ocid1.compartment...',
    summarize_metrics_data_details=oci.monitoring.models.SummarizeMetricsDataDetails(
        namespace='oci_computeagent',
        query='CpuUtilization[5m].mean()'
    )
)
```

### Key OCI Concepts

- **Tenancy**: Top-level OCI account (like AWS account)
- **Compartment**: Logical grouping (like AWS resource tags, but mandatory)
- **OCID**: Oracle Cloud Identifier (like AWS ARN)
- **Availability Domain (AD)**: Isolated datacenter (like AWS AZ)
- **Region**: Geographic area (us-ashburn-1, eu-frankfurt-1)
- **VCN**: Virtual Cloud Network (like AWS VPC)
- **NSG**: Network Security Group (like AWS Security Group)
- **Instance Principal**: VM identity for API access (better than AWS keys)
- **Service Gateway**: Free private connection to OCI services

### Cost Comparison (Typical Domo Tenant)

| Item                    | AWS Cost/Month | OCI Cost/Month | Savings |
|-------------------------|----------------|----------------|---------|
| Compute (16 OCPUs)      | $2,000         | $1,200         | 40%     |
| Storage (100GB)         | $25            | $20            | 20%     |
| Egress (50GB)           | $4,500         | $0             | 100%    |
| Load Balancer           | $30            | $20            | 33%     |
| **TOTAL**               | **$6,555**     | **$1,240**     | **81%** |

**Annual savings per tenant: ~$64K**

================================================================================
                              END OF CONTEXT
================================================================================

## How to Use This Document

1. **Share with AI coding assistants**: Copy relevant sections into Claude, Cursor, or GitHub Copilot when asking for code generation

2. **Reference during development**: Keep this open as you write migration scripts

3. **Use as code review checklist**: Ensure all code follows these patterns

4. **Update with learnings**: Add new patterns as you discover OCI-specific tricks

## Key Takeaways

‚úÖ **Always use Instance Principals** (zero-key security)
‚úÖ **Leverage Flex shapes** (resize without reboot)
‚úÖ **Optimize for free egress** (10TB/month)
‚úÖ **Use Service Gateway** (free OCI service access)
‚úÖ **Enable auto-tiering** (30-60% storage savings)
‚úÖ **Test, test, test** (validate before cutover)

## Support Resources

- OCI Documentation: https://docs.oracle.com/en-us/iaas/
- OCI Python SDK: https://oracle-cloud-infrastructure-python-sdk.readthedocs.io/
- OCI Terraform Provider: https://registry.terraform.io/providers/oracle/oci/

---

**Remember**: The goal is ZERO downtime migration with better performance and 50-70% cost savings. 

Every line of code should optimize for these outcomes.